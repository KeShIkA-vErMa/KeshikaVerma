<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Climate Change and Deforestation</title>
  <link rel="stylesheet" href="style.css"> <!-- Link to your CSS file -->
</head>
<body>
  <header>
    <nav class = "navbar">
      <a href="index.html" style="color:#ffea94; text-decoration:none;">Home</a>

    </nav>
  </header>

  <main>
    <article>
      <h1>Optipulse and WaveSync</h1>
      <p>It was an entrepreneurship bootcamp where we pitched the ideas of Wavesync and Optipulse as a team of four (team OptiWave). Since the focus was on market research, competition analysis, and business strategy, we didn't explore the deep technical aspects. However, the technical side interests me a lot. <br>
        In <b>WaveSync</b> : The microphone captures sound waves and converts them into electrical signals using an analog-to-digital converter. These signals are sent to the AI running inside the Raspberry Pi, which processes the digital audio, identifies words and sentences, and converts them into text in real time. The Raspberry Pi then sends this text to the OLED display, allowing the user to read real-time subtitles through the Wavesync attachment.
        The AI model uses patterns in sound waves to detect speech. It has been trained on thousands of words, so when it hears a sentence, it matches the sounds with known words and converts them into text.<br>
        In <b>Optipulse</b> : A camera captures real-time images of the user’s surroundings and sends them to the Raspberry Pi, which runs an AI-based object detection model. This AI model processes the image, identifies objects, and converts the detected information into speech output. The Raspberry Pi then uses a speaker to narrate the identified objects to the user, enabling visually impaired individuals to understand their environment in real time.
        The AI has been trained on thousands of images. When it sees something, it compares it to what it has learned and picks the closest match. It then speaks out the name of the object.
        </p>
      <section>
        <br>
        <h4> <center>The following is the presentation:</center></h4>
        <div style="width:100%; height:80vh; border:0; border-radius:8px; overflow:hidden;">
          <iframe
            src="Optiwave.pdf"
            title="Research PDF"
            style="width:100%; height:100%; border:0;"
          ></iframe>
        </div>
      </section>
      <br> <br>
      <h4> <center>The following is the Business Model Canvas:</center></h4>
      <img src = "Optiwave-Business Canvas.jpg" width = 800 height = 500>

    </article>
  </main>

  <footer>
    <p>© 2025 Keshika Verma. All rights reserved.</p>
    <a href="https://www.linkedin.com/in/keshika-verma/" target="_blank">LinkedIn</a>
  </footer>
</body>
</html>
